id,name,template,explanation,specialises
-9999,preamble,\newcommand{\mcauses}{\Box\kern-.5em{\implies}} \newcommand{\dimplies}{\overset{d}{\implies}} \newcommand{\cfac}{\overset{c}{\implies}},the preamble (for exporting),[]
0,qoutation,Q_{{?0 : entity}}({?1 : formula}),entity 0 claims that formula 1 is true,[]
-1,,({?0 : formula}) \mcauses ({?1 : formula}),0 is the reason why 1 is true; note that 0 and 1 could be formulas talking about the past stating that something was the reason why something was the case at some point in time,[2]
2,causal implication,({?0 : formula}) \cfac ({?1 : formula}),truth of formula 0 leads to truth of formula 1,[]
3,desire,D_{{?0 : entity}} ({?1 : formula}),truth of formula 1 is desirable for entity 0,[1;4]
4,ought,O ({?0 : formula}),formula 0 should be made true,[]
5,possibility,\diamond ({?0 : formula}),formula 0 could be true,[]
6,bad for concept,"badFor({?0 : concept}, {?1 : formula})",formula 1 is/would be bad for concept 0
-7,,({?1 : formula}) \mcauses Q_{{?0 : entity}} bad,1 is bad for entity 0,[6]
8,good for concept,"goodFor({?0 : concept}, {?1 : formula})",formula 1 is/would be good for concept 0
-9,,({?1 : formula}) \mcauses Q_{{?0 : entity}} good,1 is good for entity 0,[8]
10,belief,B_{{?0 : entity}}({?1 : formula}),entity 0 believes that formula 1 is true,[]
11,option,A_{{?0 : entity}}({?1 : action}),entity 0 has the option to perform action 1,[]
12,default implication,({?0 : formula}) \dimplies ({?1 : formula}),"when formula 0 is true, formula 1 usually is also true",[]
-13,,@_{{?0 : situation}}({?1 : formula}),in situation/point in time 0 1 was/is/would be the case; e.g. 'in the industrial revolution {1}'; so named points in time together with something that was or will be the case there,[]
-14,,({?0 : situation}) \sim ({?1 : situation}),situations 0 and 1 are similar,[]
15,similarity,({?0 : concept}) \sim ({?1 : concept}),concept 0 is similar to concept 1,[]
16,expressed preference,Q_{{?1 : entity}}(({?0 : formula}) \leq ({?2 : formula})),entity 1 prefers formula 2 over formula 0,[0;42]
-17,,biased({?0 : entity}),entity 0 is accused of being biased,[]
-18,,unqualified({?0 : entity}),entity 0 is accused of being somehow unqualified to make meaningful contributions to the dialogue; e.g. 'idiot',[]
19,entity's obligation,O ({?0 : entity}) stit ({?1 : formula}),entity 0 should make formula 1 true,[]
20,lying,Q_{{?0 : entity}} ({?1 : formula}) \land \neg B_{{?0}} ({?1}),entity 0 lies about formula 1,[27;28]
21,ad-hominem,\neg moral({?0 : entity}),entity 0 is morally bad and should be condemned; ad-hominem-catchall,[]
22,necessary condition,(\neg {?0 : formula}) \mcauses (\neg {?1 : formula}),formula 0 is necessary for formula 1,[2;27]
23,implication,({?0 : formula}) \implies ({?1 : formula}),if formula 0 then formula 1 ,[12]
24,membership,{?0 : entity} \implies {?1 : entity},entity 0 is part of entity 1,[]
25,universal,"({?0 : entity})\implies \downarrow x.\,({?1 : formula \text{ mentioning } x})",all entities in entity 0 have property 1 e.g. brexit supporters are idiots
26,position to know,"({?0 : entity})\implies \downarrow x.\,K_x ({?1 : formula}); K_{{?2 : entity}}(?1)); (?2) \implies (?0)",entity 2 is part of entity 0 whose members know formula 1
27,negation,\neg {?0 : formula},formula 0 is false,[]
-28,honesty,Q_{{?0 : entity}}({?1 : formula}) \implies B_{{?0}} ({?1}),entity 0 is claimed to be honest w.r.t. formula 1 in the sense that they say what they believe,[0;10;23]
-29,eventual result,[{?0 : action}]F({?1 : formula}),by executing action 0 truth of 1 would be eventually achieved,[]
30,action execution,E_{?0 : entity}({?1 : action}),entity 0 executed action 1,[]
-31,DES,[\{?0 : entities\};\{{?1 : \text{list of lists of }actions}\} : {?2 : entities}]({?3 : formula}),the entities 0 can collaborate to enforce truth of 3 as long as entities 2 execute one of the (joint) actions from 1,[]
32,weak universal,"({?0 : entity})\dimplies \downarrow x.\,({?1 : formula\text{ mentioning }x})",most entities in entity 0 have property 1 e.g. most brexit supporters are idiots
-33,possible causation claim,Q_{{?0 : entity}} \diamond (({?1 : formula}) \mcauses ({?2 : formula})),entity 0 claims that 1 might cause 2 to be true,[1;2;5]
34,knowledge,K_{{?0 : entity}} ({?1 : formula}),entity 0 knows that formula 1 is true,[]
35,necessary truth,\top \cfac {?0 : formula},formula 0 is necessarily true,[]
36,permission,P ({?0 : formula}),formula 0 would be morally acceptable,[]
37,qoutation of entity's obligation,"Q_{{?0 : entity}} ({?1 : entity} \implies \downarrow x.\,O (x stit ({?2 : formula}))",entity 0 states that everybody in entity 1 should make formula 2 true
38,warning,Q_{{?0 : entity}} bad({?1 : formula}),entity 0 warns of formula 1 being bad,[0;6]
39,some,"\neg(({?0 : entity} \implies \downarrow x.\,\neg {?1 : formula\text{ mentioning }x}) \lor ({0} \dimplies \downarrow x,\, {1}))",some entities in 0 have property 1,[25;27;32]
40,nobody,"\neg({?0 : entity} \implies \downarrow x.\,{?1 : formula\text{ mentioning }x})",nobody in 0 has property 1,[25;27]
41,warning of bad consequence,Q_{{?0 : entity}}(bad({?2 : formula}) \land {?1 : formula} \cfac {?2}),entity 0 warns that formula 1 has negative consequence formula 2,[0;2;6;38]
42,betterness,({?0 : formula}) \leq ({?1 : formula}),1 is/would be at least as good as 0,[]
43,concept indexed betterness,({?0 : formula}) \leq_{{?2 : concept}} ({?1 : formula}),1 is/would be at least as good for concept 2 as 0,[]
44,good,good({?0 : formula}),0 is/would be genereally good,[]
45,bad,bad({?0 : formula}),0 is/would be generally bad,[]
9999,,\top,uncategorized queries,[]
